{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-07 08:18:56,524] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/johannesthurn/gym')\n",
    "import gym\n",
    "\n",
    "# Frozen Lake Environment laden\n",
    "envFrozLake = gym.make('FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "    def every_visit_monte_carlo_prediction_V(self,policy,gamma=0.9,nb_episodes=10000):\n",
    "        N_S = np.zeros(self.env.observation_space.n)\n",
    "        V = np.zeros(self.env.observation_space.n)\n",
    "        for e in range(nb_episodes):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            X = []\n",
    "            R = [] \n",
    "            # Eine Episode erstellen \n",
    "            while not done:\n",
    "                a = policy[s]\n",
    "                X.append(s)\n",
    "                s_prime, reward, done, _ = self.env.step(a)\n",
    "                R.append(reward)\n",
    "                s = s_prime\n",
    "            # V updaten\n",
    "            self.EveryVisitMC_V(X,R,V,gamma,N_S)\n",
    "            \n",
    "        return V\n",
    "                # X0, R1, X1, R2, . . . , XT −1, RT , V \n",
    "    def EveryVisitMC_V(self,X,R,V,gamma,N_S):\n",
    "        '''\n",
    "        X, Array mit den Besuchten States, \n",
    "            X[0]  -> Start State\n",
    "            X[-1] -> State T-1\n",
    "        R, Rewards,\n",
    "            R[0]  -> Reward t1\n",
    "            R[-1] -> Reward T\n",
    "        gamma, dicount Rate\n",
    "        N_S, Array Anzahl der Besuche pro State\n",
    "        alpha, learning Rate\n",
    "            Kommentar aus Algorithms for Reinforcement Learning:\n",
    "             ->iterate-averaging is rarely used in practice<- (grund non-stationity ...)\n",
    "             Es wird Averagin zuerst implementiert\n",
    "        '''\n",
    "        _sum = 0\n",
    "        for t in reversed(range(len(X))):\n",
    "            # sum ← Rt+1 + γ * sum\n",
    "            # target[Xt] ← sum\n",
    "            _sum = R[t] + gamma * _sum\n",
    "            N_S[X[t]] += 1\n",
    "            alpha = 1/N_S[X[t]]\n",
    "            # V[Xt]←  V[Xt]  +  α * (target[Xt]−V[Xt])\n",
    "            V[X[t]] = V[X[t]] + alpha * (_sum - V[X[t]])\n",
    "            \n",
    "    def every_visit_monte_carlo_prediction_Q(self,policy,gamma=0.9,nb_episodes=10000):\n",
    "        N_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        for e in range(nb_episodes):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            X = []\n",
    "            R = [] \n",
    "            # Eine Episode erstellen \n",
    "            while not done:\n",
    "                a = policy[s]\n",
    "                s_prime, reward, done, _ = self.env.step(a)\n",
    "                X.append([s,a])\n",
    "                R.append(reward)\n",
    "                s = s_prime\n",
    "            # Q updaten\n",
    "            self.EveryVisitMC_Q(X,R,Q_SA,gamma,N_SA)\n",
    "            \n",
    "        return Q_SA\n",
    "        \n",
    "    def EveryVisitMC_Q(self,X_SA,R,Q_SA,gamma,N_SA):\n",
    "        '''\n",
    "        X, Array mit (S,Action,S_Prime), \n",
    "            X[0]  -> Start State\n",
    "            X[-1] -> State T-1\n",
    "        R, Rewards,\n",
    "            R[0]  -> Reward t1\n",
    "            R[-1] -> Reward T\n",
    "        gamma, dicount Rate\n",
    "        N_S, Array Anzahl der Besuche pro State\n",
    "        alpha, learning Rate\n",
    "            Kommentar aus Algorithms for Reinforcement Learning:\n",
    "             ->iterate-averaging is rarely used in practice<- (grund non-stationity ...)\n",
    "             Es wird Averagin zuerst implementiert\n",
    "        '''\n",
    "        _sum = 0\n",
    "        for t in reversed(range(len(X_SA))):\n",
    "            # sum ← Rt+1 + γ * sum\n",
    "            # target[Xt] ← sum\n",
    "            s,a = X_SA[t]\n",
    "            _sum = R[t] + gamma * _sum\n",
    "            N_SA[s,a] += 1\n",
    "            alpha = 1/N_SA[s,a]\n",
    "            # V[Xt]←  V[Xt]  +  α * (target[Xt]−V[Xt])\n",
    "            Q_SA[s,a] = Q_SA[s,a] + alpha * (_sum - Q_SA[s,a])\n",
    "                \n",
    "    def MonteCarlo_Control(self,gamma=0.9,nb_episodes=10000):\n",
    "        N_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        policy = np.random.randint(0,self.env.action_space.n,self.env.observation_space.n)\n",
    "        pol_old = policy\n",
    "        for e in range(1,nb_episodes+1):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            X = []\n",
    "            R = [] \n",
    "            # Eine Episode erstellen \n",
    "            while not done:\n",
    "                a = policy[s]\n",
    "                s_prime, reward, done, _ = self.env.step(a)\n",
    "                X.append([s,a,s_prime])\n",
    "                R.append(reward)\n",
    "                s = s_prime\n",
    "            # Q updaten\n",
    "            self.EveryVisitMC_Q(X,R,Q_SA,gamma,N_SA)\n",
    "            # Policy updaten\n",
    "            policy = self.policyUpdate(Q_SA,e)\n",
    "            # Zwischenergebnisse ausgeben\n",
    "            if e % 1000 == 0:\n",
    "                print('Expected Reward after ' + str(e) + ' episodes')\n",
    "                pol_new =np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)\n",
    "                print(self.evaluate(np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)))\n",
    "                if np.allclose(pol_new,pol_old):\n",
    "                    print('Policy not changed')\n",
    "                    return N_SA,Q_SA,pol_new\n",
    "                pol_old = pol_new\n",
    "                print('------------------------------------------------')\n",
    "        return N_SA,Q_SA,np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)\n",
    "            \n",
    "    def policyUpdate(self,Q_SA,k):\n",
    "        epsilon = 0.1 #1/k\n",
    "        policy = np.zeros(self.env.observation_space.n,dtype=int)\n",
    "        for s in range(self.env.observation_space.n):\n",
    "            if np.random.rand() < epsilon:\n",
    "                policy[s] = self.env.action_space.sample()\n",
    "            else:\n",
    "                mostValued = np.where(Q_SA[s] == np.amax(Q_SA[s]))[0] # [0] da np.where returns Tupel\n",
    "                choose_one = np.random.randint(0,len(mostValued),1)[0] \n",
    "                policy[s] = mostValued[choose_one]\n",
    "        return policy\n",
    "    \n",
    "    def evaluate(self,policy,trials=100):\n",
    "        rewardAll = 0\n",
    "        counter = 0\n",
    "        for i in range(trials):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            rewardTrial = 0\n",
    "            while not done:\n",
    "                state, reward, done, _ = self.env.step(policy[state])\n",
    "                rewardTrial += reward\n",
    "            rewardAll += rewardTrial\n",
    "            counter += 1\n",
    "        return rewardAll/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MonteCarlo(envFrozLake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2}\n",
    "# What is the average performance of the policy,\n",
    "# i.e. the percentage that the agent reach the goal state starting from the beginning.\n",
    "mc.evaluate(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024880640172147344"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte Carlo Prediction 1\n",
    "v = mc.every_visit_monte_carlo_prediction_V(pi,gamma=1)\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_SA\n",
      "[[ 0.          0.00942032  0.          0.        ]\n",
      " [ 0.          0.          0.00997466  0.        ]\n",
      " [ 0.          0.02368717  0.          0.        ]\n",
      " [ 0.0118944   0.          0.          0.        ]\n",
      " [ 0.          0.0120402   0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.05823837  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.02753986  0.        ]\n",
      " [ 0.07974976  0.          0.          0.        ]\n",
      " [ 0.          0.19516859  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.27971245  0.        ]\n",
      " [ 0.          0.          0.58023277  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "V\n",
      "[ 0.00942032  0.00997466  0.02368717  0.0118944   0.0120402   0.\n",
      "  0.05823837  0.          0.02753986  0.07974976  0.19516859  0.          0.\n",
      "  0.27971245  0.58023277  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Prediction 2\n",
    "q = mc.every_visit_monte_carlo_prediction_Q(pi)\n",
    "print('Q_SA')\n",
    "print(q)\n",
    "print('V')\n",
    "print(np.max(q,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward after 1000 episodes\n",
      "0.19\n",
      "------------------------------------------------\n",
      "Expected Reward after 2000 episodes\n",
      "0.15\n",
      "------------------------------------------------\n",
      "Expected Reward after 3000 episodes\n",
      "0.25\n",
      "------------------------------------------------\n",
      "Expected Reward after 4000 episodes\n",
      "0.2\n",
      "------------------------------------------------\n",
      "Expected Reward after 5000 episodes\n",
      "0.57\n",
      "------------------------------------------------\n",
      "Expected Reward after 6000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 7000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 8000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 9000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 10000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 11000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 12000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 13000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 14000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 15000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 16000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 17000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 18000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 19000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 20000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 21000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 22000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 23000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 24000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 25000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 26000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 27000 episodes\n",
      "0.81\n",
      "------------------------------------------------\n",
      "Expected Reward after 28000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 29000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 30000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 31000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 32000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 33000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 34000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 35000 episodes\n",
      "0.81\n",
      "------------------------------------------------\n",
      "Expected Reward after 36000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 37000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 38000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 39000 episodes\n",
      "0.83\n",
      "------------------------------------------------\n",
      "Expected Reward after 40000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 41000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 42000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 43000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 44000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 45000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 46000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 47000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 48000 episodes\n",
      "0.81\n",
      "------------------------------------------------\n",
      "Expected Reward after 49000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 50000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 51000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 52000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 53000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 54000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 55000 episodes\n",
      "0.61\n",
      "------------------------------------------------\n",
      "Expected Reward after 56000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 57000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 58000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 59000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 60000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 61000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 62000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 63000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 64000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 65000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 66000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 67000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 68000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 69000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 70000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 71000 episodes\n",
      "0.63\n",
      "------------------------------------------------\n",
      "Expected Reward after 72000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 73000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 74000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 75000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 76000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 77000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 78000 episodes\n",
      "0.85\n",
      "------------------------------------------------\n",
      "Expected Reward after 79000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 80000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 81000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 82000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 83000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 84000 episodes\n",
      "0.65\n",
      "------------------------------------------------\n",
      "Expected Reward after 85000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 86000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 87000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 88000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 89000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 90000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 91000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward after 92000 episodes\n",
      "0.81\n",
      "------------------------------------------------\n",
      "Expected Reward after 93000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 94000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 95000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 96000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 97000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 98000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 99000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 100000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Control\n",
    "n,q,p = mc.MonteCarlo_Control(nb_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.asarray([[1,1,1,1],[0,0,1,1]])\n",
    "np.argmax(np.random.random(b.shape) * (b.T==b.max(axis=1)).T, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 0, 0, 3, 2, 2, 0, 0, 1, 3, 3, 3, 1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,envFrozLake.action_space.n,envFrozLake.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward after 1000 episodes\n",
      "0.25\n",
      "------------------------------------------------\n",
      "Expected Reward after 2000 episodes\n",
      "0.17\n",
      "------------------------------------------------\n",
      "Expected Reward after 3000 episodes\n",
      "0.56\n",
      "------------------------------------------------\n",
      "Expected Reward after 4000 episodes\n",
      "0.51\n",
      "------------------------------------------------\n",
      "Expected Reward after 5000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 6000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 7000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 8000 episodes\n",
      "0.65\n",
      "------------------------------------------------\n",
      "Expected Reward after 9000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 10000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 11000 episodes\n",
      "0.54\n",
      "------------------------------------------------\n",
      "Expected Reward after 12000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 13000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 14000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 15000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 16000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 17000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 18000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 19000 episodes\n",
      "0.84\n",
      "------------------------------------------------\n",
      "Expected Reward after 20000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 21000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 22000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 23000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 24000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 25000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 26000 episodes\n",
      "0.63\n",
      "------------------------------------------------\n",
      "Expected Reward after 27000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 28000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 29000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 30000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 31000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 32000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 33000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 34000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 35000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 36000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 37000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 38000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 39000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 40000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 41000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 42000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 43000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 44000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 45000 episodes\n",
      "0.76\n",
      "------------------------------------------------\n",
      "Expected Reward after 46000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 47000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 48000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 49000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 50000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 51000 episodes\n",
      "0.75\n",
      "------------------------------------------------\n",
      "Expected Reward after 52000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 53000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 54000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 55000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 56000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 57000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 58000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 59000 episodes\n",
      "0.74\n",
      "------------------------------------------------\n",
      "Expected Reward after 60000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 61000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 62000 episodes\n",
      "0.65\n",
      "------------------------------------------------\n",
      "Expected Reward after 63000 episodes\n",
      "0.85\n",
      "------------------------------------------------\n",
      "Expected Reward after 64000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 65000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 66000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 67000 episodes\n",
      "0.83\n",
      "------------------------------------------------\n",
      "Expected Reward after 68000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 69000 episodes\n",
      "0.68\n",
      "------------------------------------------------\n",
      "Expected Reward after 70000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 71000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 72000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 73000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 74000 episodes\n",
      "0.64\n",
      "------------------------------------------------\n",
      "Expected Reward after 75000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 76000 episodes\n",
      "0.67\n",
      "------------------------------------------------\n",
      "Expected Reward after 77000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 78000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 79000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 80000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 81000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 82000 episodes\n",
      "0.69\n",
      "------------------------------------------------\n",
      "Expected Reward after 83000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 84000 episodes\n",
      "0.79\n",
      "------------------------------------------------\n",
      "Expected Reward after 85000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 86000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 87000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 88000 episodes\n",
      "0.7\n",
      "------------------------------------------------\n",
      "Expected Reward after 89000 episodes\n",
      "0.78\n",
      "------------------------------------------------\n",
      "Expected Reward after 90000 episodes\n",
      "0.71\n",
      "------------------------------------------------\n",
      "Expected Reward after 91000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward after 92000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 93000 episodes\n",
      "0.63\n",
      "------------------------------------------------\n",
      "Expected Reward after 94000 episodes\n",
      "0.8\n",
      "------------------------------------------------\n",
      "Expected Reward after 95000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 96000 episodes\n",
      "0.72\n",
      "------------------------------------------------\n",
      "Expected Reward after 97000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n",
      "Expected Reward after 98000 episodes\n",
      "0.66\n",
      "------------------------------------------------\n",
      "Expected Reward after 99000 episodes\n",
      "0.77\n",
      "------------------------------------------------\n",
      "Expected Reward after 100000 episodes\n",
      "0.73\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n,q,p = mc.MonteCarlo_Control(nb_episodes=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
