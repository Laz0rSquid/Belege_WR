{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-07 15:17:00,656] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/johannesthurn/gym')\n",
    "import gym\n",
    "\n",
    "# Frozen Lake Environment laden\n",
    "envFrozLake = gym.make('FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TD0(S,action,r,S_p):\n",
    "    '''\n",
    "    S is the last state,\n",
    "    action is the action taken,\n",
    "    r is the immediate reward, \n",
    "    S_P is the following State\n",
    "\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TemporalDifferenceZero:\n",
    "    \n",
    "    def __init__(self,env,alpha=0.8,gamma=0.95,epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q_SA = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        \n",
    "    def run(self,episodes=100000):\n",
    "        \n",
    "        for e in range(episodes):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            while not done:\n",
    "                a = self.getGreedyAction(s)\n",
    "                s_prime,reward,done,_ = self.env.step(a)\n",
    "                self.Q_SA[s,a] = self.Q_SA[s,a] + self.alpha*(reward + self.gamma * np.max(self.Q_SA[s_prime, : ]) - self.Q_SA[s,a])\n",
    "                s = s_prime\n",
    "                \n",
    "    def getGreedyAction(self,s):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random Action\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            mostValued = np.where(self.Q_SA[s] == np.amax(self.Q_SA[s]))[0] # [0] da np.where returns Tupel\n",
    "            choose_one = np.random.randint(0,len(mostValued),1)[0] \n",
    "            return mostValued[choose_one]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = TemporalDifferenceZero(envFrozLake, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.24317711e-01,   5.65609772e-02,   8.74065836e-02,\n",
       "          2.55576584e-01],\n",
       "       [  7.50422781e-03,   9.54072035e-03,   1.47371702e-03,\n",
       "          1.12163206e-01],\n",
       "       [  8.84574292e-03,   1.66441572e-02,   2.96632447e-02,\n",
       "          4.11438548e-02],\n",
       "       [  6.86686348e-05,   1.07238984e-02,   1.03655359e-02,\n",
       "          3.80294536e-02],\n",
       "       [  3.71401844e-01,   3.64219646e-02,   2.50489149e-01,\n",
       "          1.29694529e-01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  5.60116062e-02,   4.60974432e-12,   6.02708003e-03,\n",
       "          2.66762276e-09],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  3.00964571e-02,   9.30273535e-02,   7.16745632e-02,\n",
       "          4.31325687e-01],\n",
       "       [  1.61322285e-01,   6.08034744e-01,   1.47142173e-01,\n",
       "          1.22751421e-01],\n",
       "       [  5.19039371e-01,   1.36525639e-01,   3.78526662e-03,\n",
       "          1.77258411e-03],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  4.50814496e-02,   1.51162738e-01,   7.52343353e-01,\n",
       "          9.05594352e-02],\n",
       "       [  7.66488381e-01,   8.54293322e-01,   9.77680883e-01,\n",
       "          1.23405687e-01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.Q_SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env,policy,trials=100):\n",
    "    rewardAll = 0\n",
    "    counter = 0\n",
    "    for i in range(trials):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewardTrial = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(policy[state])\n",
    "            rewardTrial += reward\n",
    "        rewardAll += rewardTrial\n",
    "        counter += 1\n",
    "    return rewardAll/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = np.argmax(t.Q_SA, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(envFrozLake,pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLearning\n",
    "class TemporalDifferenceZero2:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "    def run(self,gamma=0.95,epsilon=0.1,episodes=100000):\n",
    "        Q_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        N_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        rAll = 0\n",
    "        for e in range(1,episodes+1):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            while not done:\n",
    "                a = self.getGreedyAction(s,Q_SA,epsilon)\n",
    "                s_prime,reward,done,_ = self.env.step(a)\n",
    "                N_SA[s,a] += 1   # Aplha nicht sinnvoll\n",
    "                alpha = 1/N_SA[s,a]    #.       Q(S_Prime) ist nun nicht von Pi abhaengig -> Q-Learning\n",
    "                Q_SA[s,a] = Q_SA[s,a] + alpha*(reward + gamma * np.max(Q_SA[s_prime, : ]) - Q_SA[s,a])\n",
    "                s = s_prime\n",
    "                rAll += reward \n",
    "            \n",
    "            if e % 10000 == 0:\n",
    "                print('Averaged Reward after ' + str(e) + ' episodes')\n",
    "                print(rAll/10000)\n",
    "                rAll = 0\n",
    "            \n",
    "        return Q_SA, np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)\n",
    "                \n",
    "    def getGreedyAction(self,s,Q_SA,epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Random Action\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            mostValued = np.where(Q_SA[s] == np.amax(Q_SA[s]))[0] # [0] da np.where returns Tupel\n",
    "            choose_one = np.random.randint(0,len(mostValued),1)[0] \n",
    "            return mostValued[choose_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2 = TemporalDifferenceZero2(envFrozLake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Reward after 10000 episodes\n",
      "0.2271\n",
      "Averaged Reward after 20000 episodes\n",
      "0.2755\n",
      "Averaged Reward after 30000 episodes\n",
      "0.2856\n",
      "Averaged Reward after 40000 episodes\n",
      "0.2876\n",
      "Averaged Reward after 50000 episodes\n",
      "0.2784\n",
      "Averaged Reward after 60000 episodes\n",
      "0.2823\n",
      "Averaged Reward after 70000 episodes\n",
      "0.279\n",
      "Averaged Reward after 80000 episodes\n",
      "0.2877\n",
      "Averaged Reward after 90000 episodes\n",
      "0.2822\n",
      "Averaged Reward after 100000 episodes\n",
      "0.2777\n",
      "Averaged Reward after 110000 episodes\n",
      "0.2868\n",
      "Averaged Reward after 120000 episodes\n",
      "0.2805\n",
      "Averaged Reward after 130000 episodes\n",
      "0.2792\n",
      "Averaged Reward after 140000 episodes\n",
      "0.2873\n",
      "Averaged Reward after 150000 episodes\n",
      "0.2779\n",
      "Averaged Reward after 160000 episodes\n",
      "0.2755\n",
      "Averaged Reward after 170000 episodes\n",
      "0.2857\n",
      "Averaged Reward after 180000 episodes\n",
      "0.2871\n",
      "Averaged Reward after 190000 episodes\n",
      "0.2857\n",
      "Averaged Reward after 200000 episodes\n",
      "0.2853\n",
      "Averaged Reward after 210000 episodes\n",
      "0.2892\n",
      "Averaged Reward after 220000 episodes\n",
      "0.2776\n",
      "Averaged Reward after 230000 episodes\n",
      "0.2813\n",
      "Averaged Reward after 240000 episodes\n",
      "0.2853\n",
      "Averaged Reward after 250000 episodes\n",
      "0.2832\n",
      "Averaged Reward after 260000 episodes\n",
      "0.2841\n",
      "Averaged Reward after 270000 episodes\n",
      "0.2829\n",
      "Averaged Reward after 280000 episodes\n",
      "0.2861\n",
      "Averaged Reward after 290000 episodes\n",
      "0.2836\n",
      "Averaged Reward after 300000 episodes\n",
      "0.2804\n",
      "Averaged Reward after 310000 episodes\n",
      "0.2798\n",
      "Averaged Reward after 320000 episodes\n",
      "0.2901\n",
      "Averaged Reward after 330000 episodes\n",
      "0.2774\n",
      "Averaged Reward after 340000 episodes\n",
      "0.2951\n",
      "Averaged Reward after 350000 episodes\n",
      "0.2826\n",
      "Averaged Reward after 360000 episodes\n",
      "0.2844\n",
      "Averaged Reward after 370000 episodes\n",
      "0.2777\n",
      "Averaged Reward after 380000 episodes\n",
      "0.2798\n",
      "Averaged Reward after 390000 episodes\n",
      "0.2877\n",
      "Averaged Reward after 400000 episodes\n",
      "0.2775\n",
      "Averaged Reward after 410000 episodes\n",
      "0.2797\n",
      "Averaged Reward after 420000 episodes\n",
      "0.2956\n",
      "Averaged Reward after 430000 episodes\n",
      "0.281\n",
      "Averaged Reward after 440000 episodes\n",
      "0.2839\n",
      "Averaged Reward after 450000 episodes\n",
      "0.2778\n",
      "Averaged Reward after 460000 episodes\n",
      "0.2792\n",
      "Averaged Reward after 470000 episodes\n",
      "0.2871\n",
      "Averaged Reward after 480000 episodes\n",
      "0.2818\n",
      "Averaged Reward after 490000 episodes\n",
      "0.2705\n",
      "Averaged Reward after 500000 episodes\n",
      "0.2854\n"
     ]
    }
   ],
   "source": [
    "q, pol = t2.run(gamma=0.9,episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(envFrozLake,pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sarsa -> on Policy\n",
    "class TemporalDifferenceZero3:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "    def run(self,gamma=0.95,epsilon=0.05,episodes=100000):\n",
    "        Q_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        N_SA = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            done = False\n",
    "            s = self.env.reset()\n",
    "            while not done:\n",
    "                a = self.getGreedyAction(s,Q_SA,epsilon)\n",
    "                s_prime,reward,done,_ = self.env.step(a)\n",
    "                N_SA[s,a] += 1\n",
    "                alpha = 0.02 #1/e - ganz schlecht #1/N_SA[s,a]\n",
    "                a_prime = self.getGreedyAction(s_prime,Q_SA,epsilon)\n",
    "                Q_SA[s,a] = Q_SA[s,a] + alpha*(reward + gamma * Q_SA[s_prime,a_prime] - Q_SA[s,a])\n",
    "                s = s_prime\n",
    "            \n",
    "            if (e-1) % 1000 == 0:\n",
    "                print('Expected Reward after ' + str(e) + ' episodes')\n",
    "                pol_new =np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)\n",
    "                print(evaluate(self.env,pol_new))\n",
    "            \n",
    "        return Q_SA, np.argmax(np.random.random(Q_SA.shape) * (Q_SA.T==Q_SA.max(axis=1)).T, axis=1)\n",
    "                \n",
    "    def getGreedyAction(self,s,Q_SA,epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Random Action\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            mostValued = np.where(Q_SA[s] == np.amax(Q_SA[s]))[0] # [0] da np.where returns Tupel\n",
    "            choose_one = np.random.randint(0,len(mostValued),1)[0] \n",
    "            return mostValued[choose_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t3 = TemporalDifferenceZero3(envFrozLake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward after 1 episodes\n",
      "0.0\n",
      "Expected Reward after 1001 episodes\n",
      "0.0\n",
      "Expected Reward after 2001 episodes\n",
      "0.04\n",
      "Expected Reward after 3001 episodes\n",
      "0.01\n",
      "Expected Reward after 4001 episodes\n",
      "0.07\n",
      "Expected Reward after 5001 episodes\n",
      "0.14\n",
      "Expected Reward after 6001 episodes\n",
      "0.13\n",
      "Expected Reward after 7001 episodes\n",
      "0.1\n",
      "Expected Reward after 8001 episodes\n",
      "0.15\n",
      "Expected Reward after 9001 episodes\n",
      "0.53\n",
      "Expected Reward after 10001 episodes\n",
      "0.68\n",
      "Expected Reward after 11001 episodes\n",
      "0.65\n",
      "Expected Reward after 12001 episodes\n",
      "0.6\n",
      "Expected Reward after 13001 episodes\n",
      "0.73\n",
      "Expected Reward after 14001 episodes\n",
      "0.66\n",
      "Expected Reward after 15001 episodes\n",
      "0.74\n",
      "Expected Reward after 16001 episodes\n",
      "0.69\n",
      "Expected Reward after 17001 episodes\n",
      "0.77\n",
      "Expected Reward after 18001 episodes\n",
      "0.73\n",
      "Expected Reward after 19001 episodes\n",
      "0.7\n",
      "Expected Reward after 20001 episodes\n",
      "0.68\n",
      "Expected Reward after 21001 episodes\n",
      "0.27\n",
      "Expected Reward after 22001 episodes\n",
      "0.64\n",
      "Expected Reward after 23001 episodes\n",
      "0.76\n",
      "Expected Reward after 24001 episodes\n",
      "0.76\n",
      "Expected Reward after 25001 episodes\n",
      "0.59\n",
      "Expected Reward after 26001 episodes\n",
      "0.75\n",
      "Expected Reward after 27001 episodes\n",
      "0.6\n",
      "Expected Reward after 28001 episodes\n",
      "0.73\n",
      "Expected Reward after 29001 episodes\n",
      "0.69\n",
      "Expected Reward after 30001 episodes\n",
      "0.75\n",
      "Expected Reward after 31001 episodes\n",
      "0.59\n",
      "Expected Reward after 32001 episodes\n",
      "0.83\n",
      "Expected Reward after 33001 episodes\n",
      "0.58\n",
      "Expected Reward after 34001 episodes\n",
      "0.78\n",
      "Expected Reward after 35001 episodes\n",
      "0.72\n",
      "Expected Reward after 36001 episodes\n",
      "0.58\n",
      "Expected Reward after 37001 episodes\n",
      "0.55\n",
      "Expected Reward after 38001 episodes\n",
      "0.71\n",
      "Expected Reward after 39001 episodes\n",
      "0.76\n",
      "Expected Reward after 40001 episodes\n",
      "0.77\n",
      "Expected Reward after 41001 episodes\n",
      "0.6\n",
      "Expected Reward after 42001 episodes\n",
      "0.66\n",
      "Expected Reward after 43001 episodes\n",
      "0.73\n",
      "Expected Reward after 44001 episodes\n",
      "0.76\n",
      "Expected Reward after 45001 episodes\n",
      "0.75\n",
      "Expected Reward after 46001 episodes\n",
      "0.67\n",
      "Expected Reward after 47001 episodes\n",
      "0.56\n",
      "Expected Reward after 48001 episodes\n",
      "0.76\n",
      "Expected Reward after 49001 episodes\n",
      "0.76\n",
      "Expected Reward after 50001 episodes\n",
      "0.5\n",
      "Expected Reward after 51001 episodes\n",
      "0.74\n",
      "Expected Reward after 52001 episodes\n",
      "0.74\n",
      "Expected Reward after 53001 episodes\n",
      "0.75\n",
      "Expected Reward after 54001 episodes\n",
      "0.73\n",
      "Expected Reward after 55001 episodes\n",
      "0.78\n",
      "Expected Reward after 56001 episodes\n",
      "0.7\n",
      "Expected Reward after 57001 episodes\n",
      "0.71\n",
      "Expected Reward after 58001 episodes\n",
      "0.71\n",
      "Expected Reward after 59001 episodes\n",
      "0.75\n",
      "Expected Reward after 60001 episodes\n",
      "0.72\n",
      "Expected Reward after 61001 episodes\n",
      "0.73\n",
      "Expected Reward after 62001 episodes\n",
      "0.67\n",
      "Expected Reward after 63001 episodes\n",
      "0.8\n",
      "Expected Reward after 64001 episodes\n",
      "0.66\n",
      "Expected Reward after 65001 episodes\n",
      "0.59\n",
      "Expected Reward after 66001 episodes\n",
      "0.7\n",
      "Expected Reward after 67001 episodes\n",
      "0.74\n",
      "Expected Reward after 68001 episodes\n",
      "0.71\n",
      "Expected Reward after 69001 episodes\n",
      "0.69\n",
      "Expected Reward after 70001 episodes\n",
      "0.69\n",
      "Expected Reward after 71001 episodes\n",
      "0.71\n",
      "Expected Reward after 72001 episodes\n",
      "0.7\n",
      "Expected Reward after 73001 episodes\n",
      "0.75\n",
      "Expected Reward after 74001 episodes\n",
      "0.7\n",
      "Expected Reward after 75001 episodes\n",
      "0.75\n",
      "Expected Reward after 76001 episodes\n",
      "0.57\n",
      "Expected Reward after 77001 episodes\n",
      "0.69\n",
      "Expected Reward after 78001 episodes\n",
      "0.79\n",
      "Expected Reward after 79001 episodes\n",
      "0.7\n",
      "Expected Reward after 80001 episodes\n",
      "0.72\n",
      "Expected Reward after 81001 episodes\n",
      "0.64\n",
      "Expected Reward after 82001 episodes\n",
      "0.74\n",
      "Expected Reward after 83001 episodes\n",
      "0.72\n",
      "Expected Reward after 84001 episodes\n",
      "0.73\n",
      "Expected Reward after 85001 episodes\n",
      "0.71\n",
      "Expected Reward after 86001 episodes\n",
      "0.73\n",
      "Expected Reward after 87001 episodes\n",
      "0.72\n",
      "Expected Reward after 88001 episodes\n",
      "0.78\n",
      "Expected Reward after 89001 episodes\n",
      "0.72\n",
      "Expected Reward after 90001 episodes\n",
      "0.77\n",
      "Expected Reward after 91001 episodes\n",
      "0.73\n",
      "Expected Reward after 92001 episodes\n",
      "0.72\n",
      "Expected Reward after 93001 episodes\n",
      "0.65\n",
      "Expected Reward after 94001 episodes\n",
      "0.65\n",
      "Expected Reward after 95001 episodes\n",
      "0.73\n",
      "Expected Reward after 96001 episodes\n",
      "0.64\n",
      "Expected Reward after 97001 episodes\n",
      "0.75\n",
      "Expected Reward after 98001 episodes\n",
      "0.75\n",
      "Expected Reward after 99001 episodes\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "q, pol = t3.run(gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(envFrozLake,pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 3, 0, 1, 2, 1, 3, 1, 0, 3, 3, 2, 1, 3])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14827263,  0.13735084,  0.13826603,  0.13072047],\n",
       "       [ 0.08030282,  0.08344967,  0.07712953,  0.12773562],\n",
       "       [ 0.12870363,  0.11882963,  0.12006722,  0.11399978],\n",
       "       [ 0.02125939,  0.02389113,  0.0299297 ,  0.10909998],\n",
       "       [ 0.17830151,  0.11123817,  0.11725045,  0.09993456],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.14037805,  0.11877892,  0.16442737,  0.02518008],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.11979872,  0.17256026,  0.16758132,  0.24122842],\n",
       "       [ 0.2147155 ,  0.35557153,  0.25344186,  0.20779372],\n",
       "       [ 0.36820489,  0.29373512,  0.3108812 ,  0.16224576],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.21581934,  0.3535727 ,  0.46847   ,  0.28152295],\n",
       "       [ 0.49758102,  0.68940032,  0.60770646,  0.58204025],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
