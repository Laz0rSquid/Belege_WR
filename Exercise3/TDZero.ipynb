{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-16 13:10:51,786] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "env = gym.make('FrozenLake-v0')\n",
    "pi = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TDZero:\n",
    "    \n",
    "    def __init__(self,env,alpha=0.05,gamma=0.99,epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_sa = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def run(self,episodes=10000):\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.get_epsilon_greedy_action(state)\n",
    "                next_state,reward,done,_ = self.env.step(action)\n",
    "                self.q_sa[state,action] = self.q_sa[state,action] + self.alpha*(reward + self.gamma * np.max(self.q_sa[next_state, : ]) - self.q_sa[state,action])\n",
    "                state = next_state\n",
    "                \n",
    "    def get_epsilon_greedy_action(self,state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Return greedy action\n",
    "            greedy = np.where(self.q_sa[state] == np.amax(self.q_sa[state]))[0]\n",
    "            select = np.random.randint(0,len(greedy),1)[0] \n",
    "            return greedy[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdz = TDZero(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdz.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51025902,  0.49281263,  0.48164008,  0.49347   ],\n",
       "       [ 0.31986352,  0.23431611,  0.27712259,  0.4475331 ],\n",
       "       [ 0.37752282,  0.27988847,  0.24019309,  0.25492194],\n",
       "       [ 0.03480865,  0.13571008,  0.00960432,  0.08385311],\n",
       "       [ 0.52892773,  0.37836702,  0.38925102,  0.36177912],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.35205438,  0.13860158,  0.24467419,  0.08614557],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.35333473,  0.36828908,  0.36438721,  0.56432136],\n",
       "       [ 0.51392118,  0.62887009,  0.52859803,  0.3259483 ],\n",
       "       [ 0.64781015,  0.46964517,  0.33018241,  0.30104954],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.42265795,  0.52903203,  0.69153769,  0.46977955],\n",
       "       [ 0.74050786,  0.89184757,  0.77824732,  0.75820722],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdz.q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env,policy,trials=100):\n",
    "    sum_rewards = 0\n",
    "    n = 0\n",
    "    for i in range(trials):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        reward_i = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(policy[state])\n",
    "            reward_i += reward\n",
    "        sum_rewards += reward_i\n",
    "        n += 1\n",
    "    return sum_rewards/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pol = np.argmax(td0.q_sa, axis=1)\n",
    "# evaluate(env,pol)\n",
    "evaluate(env,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QLearning\n",
    "class TDZeroQ:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def run(self,alpha=0.05,gamma=0.99,epsilon=0.1,episodes=10000):\n",
    "        q_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        n_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        sum_rewards = 0\n",
    "        for e in range(1,episodes+1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.get_epsilon_greedy_action(state,q_sa,epsilon)\n",
    "                next_state,reward,done,_ = self.env.step(action)\n",
    "                n_sa[state,action] += 1\n",
    "                alpha = 1/n_sa[state,action]\n",
    "                q_sa[state,action] = q_sa[state,action] + alpha*(reward + gamma * np.max(q_sa[next_state, : ]) - q_sa[state,action])\n",
    "                state = next_state\n",
    "                sum_rewards += reward \n",
    "            \n",
    "            if e % 10000 == 0:\n",
    "                print('Averaged Reward after ' + str(e) + ' episodes')\n",
    "                print(sum_rewards/10000)\n",
    "                sum_rewards = 0\n",
    "            \n",
    "        return q_sa, np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "                \n",
    "    def get_epsilon_greedy_action(self,state,q_sa,epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Random exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Return greedy action\n",
    "            greedy = np.where(q_sa[state] == np.amax(q_sa[state]))[0]\n",
    "            select = np.random.randint(0,len(greedy),1)[0] \n",
    "            return greedy[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdz_q = TDZeroQ(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Reward after 10000 episodes\n",
      "0.1763\n",
      "Averaged Reward after 20000 episodes\n",
      "0.2866\n",
      "Averaged Reward after 30000 episodes\n",
      "0.2807\n",
      "Averaged Reward after 40000 episodes\n",
      "0.2874\n",
      "Averaged Reward after 50000 episodes\n",
      "0.2807\n",
      "Averaged Reward after 60000 episodes\n",
      "0.2811\n",
      "Averaged Reward after 70000 episodes\n",
      "0.2917\n",
      "Averaged Reward after 80000 episodes\n",
      "0.2802\n",
      "Averaged Reward after 90000 episodes\n",
      "0.2829\n",
      "Averaged Reward after 100000 episodes\n",
      "0.2864\n",
      "Averaged Reward after 110000 episodes\n",
      "0.2848\n",
      "Averaged Reward after 120000 episodes\n",
      "0.2891\n",
      "Averaged Reward after 130000 episodes\n",
      "0.2848\n",
      "Averaged Reward after 140000 episodes\n",
      "0.2775\n",
      "Averaged Reward after 150000 episodes\n",
      "0.2814\n",
      "Averaged Reward after 160000 episodes\n",
      "0.2781\n",
      "Averaged Reward after 170000 episodes\n",
      "0.2809\n",
      "Averaged Reward after 180000 episodes\n",
      "0.277\n",
      "Averaged Reward after 190000 episodes\n",
      "0.275\n",
      "Averaged Reward after 200000 episodes\n",
      "0.2833\n",
      "Averaged Reward after 210000 episodes\n",
      "0.2857\n",
      "Averaged Reward after 220000 episodes\n",
      "0.2782\n",
      "Averaged Reward after 230000 episodes\n",
      "0.2864\n",
      "Averaged Reward after 240000 episodes\n",
      "0.2899\n",
      "Averaged Reward after 250000 episodes\n",
      "0.2819\n",
      "Averaged Reward after 260000 episodes\n",
      "0.2916\n",
      "Averaged Reward after 270000 episodes\n",
      "0.2769\n",
      "Averaged Reward after 280000 episodes\n",
      "0.2847\n",
      "Averaged Reward after 290000 episodes\n",
      "0.2821\n",
      "Averaged Reward after 300000 episodes\n",
      "0.2829\n",
      "Averaged Reward after 310000 episodes\n",
      "0.2752\n",
      "Averaged Reward after 320000 episodes\n",
      "0.2832\n",
      "Averaged Reward after 330000 episodes\n",
      "0.2849\n",
      "Averaged Reward after 340000 episodes\n",
      "0.2693\n",
      "Averaged Reward after 350000 episodes\n",
      "0.2795\n",
      "Averaged Reward after 360000 episodes\n",
      "0.2785\n",
      "Averaged Reward after 370000 episodes\n",
      "0.2837\n",
      "Averaged Reward after 380000 episodes\n",
      "0.2871\n",
      "Averaged Reward after 390000 episodes\n",
      "0.2787\n",
      "Averaged Reward after 400000 episodes\n",
      "0.2789\n",
      "Averaged Reward after 410000 episodes\n",
      "0.2773\n",
      "Averaged Reward after 420000 episodes\n",
      "0.2893\n",
      "Averaged Reward after 430000 episodes\n",
      "0.2864\n",
      "Averaged Reward after 440000 episodes\n",
      "0.2796\n",
      "Averaged Reward after 450000 episodes\n",
      "0.2784\n",
      "Averaged Reward after 460000 episodes\n",
      "0.281\n",
      "Averaged Reward after 470000 episodes\n",
      "0.2735\n",
      "Averaged Reward after 480000 episodes\n",
      "0.2791\n",
      "Averaged Reward after 490000 episodes\n",
      "0.2803\n",
      "Averaged Reward after 500000 episodes\n",
      "0.2833\n"
     ]
    }
   ],
   "source": [
    "q, pi = tdz_q.run(gamma=0.9,episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SARSA\n",
    "class TDZeroSARSA:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "    def run(self,alpha=0.05,gamma=0.99,epsilon=0.1,episodes=100000):\n",
    "        q_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        n_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        for e in range(1,episodes+1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.get_epsilon_greedy_action(state,q_sa,epsilon)\n",
    "                next_state,reward,done,_ = self.env.step(action)\n",
    "                n_sa[state,action] += 1\n",
    "                alpha = 0.02\n",
    "                next_action = self.get_epsilon_greedy_action(next_state,q_sa,epsilon)\n",
    "                q_sa[state,action] = q_sa[state,action] + alpha*(reward + gamma * np.max(q_sa[next_state, : ]) - q_sa[state,action])\n",
    "                state = next_state\n",
    "            \n",
    "            if (e-1) % 1000 == 0:\n",
    "                print('Averaged Reward after ' + str(e) + ' episodes')\n",
    "                new_policy = np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "                print(evaluate(self.env, new_policy))\n",
    "            \n",
    "        return q_sa, np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "                \n",
    "    def get_epsilon_greedy_action(self,state,q_sa,epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Random exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Return greedy action\n",
    "            greedy = np.where(q_sa[state] == np.amax(q_sa[state]))[0]\n",
    "            select = np.random.randint(0,len(greedy),1)[0] \n",
    "            return greedy[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdz_sarsa = TDZeroSARSA(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Reward after 1 episodes\n",
      "0.0\n",
      "Averaged Reward after 1001 episodes\n",
      "0.03\n",
      "Averaged Reward after 2001 episodes\n",
      "0.05\n",
      "Averaged Reward after 3001 episodes\n",
      "0.06\n",
      "Averaged Reward after 4001 episodes\n",
      "0.18\n",
      "Averaged Reward after 5001 episodes\n",
      "0.24\n",
      "Averaged Reward after 6001 episodes\n",
      "0.47\n",
      "Averaged Reward after 7001 episodes\n",
      "0.47\n",
      "Averaged Reward after 8001 episodes\n",
      "0.53\n",
      "Averaged Reward after 9001 episodes\n",
      "0.42\n",
      "Averaged Reward after 10001 episodes\n",
      "0.73\n",
      "Averaged Reward after 11001 episodes\n",
      "0.75\n",
      "Averaged Reward after 12001 episodes\n",
      "0.7\n",
      "Averaged Reward after 13001 episodes\n",
      "0.76\n",
      "Averaged Reward after 14001 episodes\n",
      "0.69\n",
      "Averaged Reward after 15001 episodes\n",
      "0.75\n",
      "Averaged Reward after 16001 episodes\n",
      "0.76\n",
      "Averaged Reward after 17001 episodes\n",
      "0.65\n",
      "Averaged Reward after 18001 episodes\n",
      "0.8\n",
      "Averaged Reward after 19001 episodes\n",
      "0.73\n",
      "Averaged Reward after 20001 episodes\n",
      "0.68\n",
      "Averaged Reward after 21001 episodes\n",
      "0.74\n",
      "Averaged Reward after 22001 episodes\n",
      "0.71\n",
      "Averaged Reward after 23001 episodes\n",
      "0.75\n",
      "Averaged Reward after 24001 episodes\n",
      "0.66\n",
      "Averaged Reward after 25001 episodes\n",
      "0.88\n",
      "Averaged Reward after 26001 episodes\n",
      "0.76\n",
      "Averaged Reward after 27001 episodes\n",
      "0.77\n",
      "Averaged Reward after 28001 episodes\n",
      "0.74\n",
      "Averaged Reward after 29001 episodes\n",
      "0.68\n",
      "Averaged Reward after 30001 episodes\n",
      "0.66\n",
      "Averaged Reward after 31001 episodes\n",
      "0.78\n",
      "Averaged Reward after 32001 episodes\n",
      "0.7\n",
      "Averaged Reward after 33001 episodes\n",
      "0.72\n",
      "Averaged Reward after 34001 episodes\n",
      "0.78\n",
      "Averaged Reward after 35001 episodes\n",
      "0.68\n",
      "Averaged Reward after 36001 episodes\n",
      "0.69\n",
      "Averaged Reward after 37001 episodes\n",
      "0.76\n",
      "Averaged Reward after 38001 episodes\n",
      "0.69\n",
      "Averaged Reward after 39001 episodes\n",
      "0.71\n",
      "Averaged Reward after 40001 episodes\n",
      "0.69\n",
      "Averaged Reward after 41001 episodes\n",
      "0.73\n",
      "Averaged Reward after 42001 episodes\n",
      "0.66\n",
      "Averaged Reward after 43001 episodes\n",
      "0.7\n",
      "Averaged Reward after 44001 episodes\n",
      "0.78\n",
      "Averaged Reward after 45001 episodes\n",
      "0.75\n",
      "Averaged Reward after 46001 episodes\n",
      "0.78\n",
      "Averaged Reward after 47001 episodes\n",
      "0.71\n",
      "Averaged Reward after 48001 episodes\n",
      "0.66\n",
      "Averaged Reward after 49001 episodes\n",
      "0.75\n",
      "Averaged Reward after 50001 episodes\n",
      "0.77\n",
      "Averaged Reward after 51001 episodes\n",
      "0.75\n",
      "Averaged Reward after 52001 episodes\n",
      "0.8\n",
      "Averaged Reward after 53001 episodes\n",
      "0.72\n",
      "Averaged Reward after 54001 episodes\n",
      "0.73\n",
      "Averaged Reward after 55001 episodes\n",
      "0.7\n",
      "Averaged Reward after 56001 episodes\n",
      "0.7\n",
      "Averaged Reward after 57001 episodes\n",
      "0.65\n",
      "Averaged Reward after 58001 episodes\n",
      "0.75\n",
      "Averaged Reward after 59001 episodes\n",
      "0.75\n",
      "Averaged Reward after 60001 episodes\n",
      "0.73\n",
      "Averaged Reward after 61001 episodes\n",
      "0.74\n",
      "Averaged Reward after 62001 episodes\n",
      "0.74\n",
      "Averaged Reward after 63001 episodes\n",
      "0.76\n",
      "Averaged Reward after 64001 episodes\n",
      "0.77\n",
      "Averaged Reward after 65001 episodes\n",
      "0.71\n",
      "Averaged Reward after 66001 episodes\n",
      "0.63\n",
      "Averaged Reward after 67001 episodes\n",
      "0.74\n",
      "Averaged Reward after 68001 episodes\n",
      "0.75\n",
      "Averaged Reward after 69001 episodes\n",
      "0.76\n",
      "Averaged Reward after 70001 episodes\n",
      "0.68\n",
      "Averaged Reward after 71001 episodes\n",
      "0.74\n",
      "Averaged Reward after 72001 episodes\n",
      "0.75\n",
      "Averaged Reward after 73001 episodes\n",
      "0.62\n",
      "Averaged Reward after 74001 episodes\n",
      "0.77\n",
      "Averaged Reward after 75001 episodes\n",
      "0.75\n",
      "Averaged Reward after 76001 episodes\n",
      "0.7\n",
      "Averaged Reward after 77001 episodes\n",
      "0.73\n",
      "Averaged Reward after 78001 episodes\n",
      "0.64\n",
      "Averaged Reward after 79001 episodes\n",
      "0.81\n",
      "Averaged Reward after 80001 episodes\n",
      "0.66\n",
      "Averaged Reward after 81001 episodes\n",
      "0.71\n",
      "Averaged Reward after 82001 episodes\n",
      "0.75\n",
      "Averaged Reward after 83001 episodes\n",
      "0.73\n",
      "Averaged Reward after 84001 episodes\n",
      "0.71\n",
      "Averaged Reward after 85001 episodes\n",
      "0.75\n",
      "Averaged Reward after 86001 episodes\n",
      "0.79\n",
      "Averaged Reward after 87001 episodes\n",
      "0.71\n",
      "Averaged Reward after 88001 episodes\n",
      "0.74\n",
      "Averaged Reward after 89001 episodes\n",
      "0.74\n",
      "Averaged Reward after 90001 episodes\n",
      "0.68\n",
      "Averaged Reward after 91001 episodes\n",
      "0.69\n",
      "Averaged Reward after 92001 episodes\n",
      "0.7\n",
      "Averaged Reward after 93001 episodes\n",
      "0.8\n",
      "Averaged Reward after 94001 episodes\n",
      "0.62\n",
      "Averaged Reward after 95001 episodes\n",
      "0.69\n",
      "Averaged Reward after 96001 episodes\n",
      "0.61\n",
      "Averaged Reward after 97001 episodes\n",
      "0.71\n",
      "Averaged Reward after 98001 episodes\n",
      "0.7\n",
      "Averaged Reward after 99001 episodes\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "q, pi = tdz_sarsa.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 0, 0, 2, 2, 1, 3, 1, 0, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5409863 ,  0.5142835 ,  0.5109796 ,  0.50515038],\n",
       "       [ 0.3746719 ,  0.30424755,  0.26857674,  0.46560099],\n",
       "       [ 0.40003723,  0.32396464,  0.3006988 ,  0.31704239],\n",
       "       [ 0.1760727 ,  0.13680972,  0.08807234,  0.14432618],\n",
       "       [ 0.55962609,  0.38828871,  0.35343693,  0.4031182 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.30196425,  0.18718252,  0.32699659,  0.14156722],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.36325864,  0.44866339,  0.44806309,  0.58973894],\n",
       "       [ 0.4033352 ,  0.64322845,  0.40870175,  0.37107211],\n",
       "       [ 0.64555936,  0.53369552,  0.42501489,  0.29117539],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.44207545,  0.57377942,  0.74320409,  0.46465264],\n",
       "       [ 0.71734577,  0.85649115,  0.80398268,  0.75661743],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
