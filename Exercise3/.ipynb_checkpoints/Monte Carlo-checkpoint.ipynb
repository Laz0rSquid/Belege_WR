{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import logging\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-16 13:25:13,611] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state  = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def act(action):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print (state)\n",
    "    env.render()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "act(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "random action: 2\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "random action: 1\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "random action: 2\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "random action: 0\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "random action: 2\n",
      "12 0.0 True {'prob': 0.3333333333333333}\n",
      "return: 0.0\n"
     ]
    }
   ],
   "source": [
    "state  = env.reset()\n",
    "print (state)\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = pi[state]\n",
    "    print (\"random action:\", action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print (state, reward, done, info)\n",
    "    if done:\n",
    "        print (\"return:\", reward) # return for all visited states is here last reward\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# V(s)\n",
    "def every_visit_monte_carlo_prediction_v(mc_policy, mc_env, target_state, nb_episodes=10000, discount=0.9):\n",
    "    n_s = np.zeros(mc_env.observation_space.n)\n",
    "    v = np.zeros(mc_env.observation_space.n)\n",
    "    for i in range(1, nb_episodes + 1):\n",
    "        episode = []\n",
    "        state = mc_env.reset()\n",
    "        done = False\n",
    "        # V\n",
    "        while not done:\n",
    "            action = mc_policy[state]\n",
    "            next_state, reward, done, _ = mc_env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        every_visit_mc_v(episode,v,discount,n_s)\n",
    "    return v\n",
    "\n",
    "def every_visit_mc_v(episode,v,discount,n_s):\n",
    "        sum = 0\n",
    "        for x in reversed(range(len(episode))):\n",
    "            s,a,r = episode[x]\n",
    "            sum = r + discount * sum\n",
    "            n_s[[s,a]] += 1\n",
    "            alpha = 1/n_s[[s,a]]\n",
    "            v[[s,a]] = v[[s,a]] + alpha * (sum - v[[s,a]])\n",
    "\n",
    "# Q(s,a)\n",
    "def every_visit_monte_carlo_prediction_q(mc_policy, mc_env, target_state, nb_episodes=10000, discount=0.9):\n",
    "    n_sa = np.zeros([mc_env.observation_space.n, mc_env.action_space.n])\n",
    "    q_sa = np.zeros([mc_env.observation_space.n, mc_env.action_space.n])\n",
    "    for e in range(nb_episodes):\n",
    "        done = False\n",
    "        state = mc_env.reset()\n",
    "        episode = []\n",
    "        # Q\n",
    "        while not done:\n",
    "            action = mc_policy[state]\n",
    "            next_state, reward, done, _ = mc_env.step(action)\n",
    "            episode.append([state,action,reward])\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        # Q updaten\n",
    "        every_visit_mc_q(episode,q_sa,discount,n_sa)     \n",
    "    return q_sa\n",
    "        \n",
    "def every_visit_mc_q(episode,q_sa,discount,n_sa):\n",
    "    _sum = 0\n",
    "    for x in reversed(range(len(episode))):\n",
    "        s,a,r = episode[x]\n",
    "        _sum = r + discount * _sum\n",
    "        n_sa[s,a] += 1\n",
    "        alpha = 1/n_sa[s,a]\n",
    "        q_sa[s,a] = q_sa[s,a] + alpha * (_sum - q_sa[s,a])\n",
    "\n",
    "# control\n",
    "def montecarlo_control(mc_env,discount=0.9,nb_episodes=10000):\n",
    "    n_sa = np.zeros([mc_env.observation_space.n, mc_env.action_space.n])\n",
    "    q_sa = np.zeros([mc_env.observation_space.n, mc_env.action_space.n])\n",
    "    policy = np.random.randint(0,mc_env.action_space.n,mc_env.observation_space.n)\n",
    "    pol_old = policy\n",
    "    for e in range(1,nb_episodes+1):\n",
    "        done = False\n",
    "        state = mc_env.reset()\n",
    "        episode = [] \n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _ = mc_env.step(action)\n",
    "            episode.append([state,action,reward])\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        every_visit_mc_q(episode,q_sa,discount,n_sa)\n",
    "        policy = policy_update(mc_env,q_sa,e)\n",
    "        if e % 1000 == 0:\n",
    "            print('Expected Reward post episode ' + str(e) + '')\n",
    "            pol_new =np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "            print(evaluate(mc_env, pol_new))\n",
    "            if np.allclose(pol_new,pol_old):\n",
    "                print(\"Policy hasn't changed\")\n",
    "                return n_sa,q_sa,pol_new\n",
    "            pol_old = pol_new\n",
    "    return n_sa,q_sa,np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "\n",
    "def policy_update(mc_env,q_sa,k):\n",
    "    epsilon = 0.1\n",
    "    policy = np.zeros(mc_env.observation_space.n,dtype=int)\n",
    "    for s in range(mc_env.observation_space.n):\n",
    "        if np.random.rand() < epsilon:\n",
    "            policy[s] = mc_env.action_space.sample()\n",
    "        else:\n",
    "            mostValued = np.where(q_sa[s] == np.amax(q_sa[s]))[0] # [0] da np.where returns Tupel\n",
    "            choose_one = np.random.randint(0,len(mostValued),1)[0] \n",
    "            policy[s] = mostValued[choose_one]\n",
    "    return policy\n",
    "    \n",
    "def evaluate(mc_env, policy,trials=100):\n",
    "    rewardAll = 0\n",
    "    counter = 0\n",
    "    for i in range(trials):\n",
    "        state = mc_env.reset()\n",
    "        done = False\n",
    "        rewardTrial = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = mc_env.step(policy[state])\n",
    "            rewardTrial += reward\n",
    "        rewardAll += rewardTrial\n",
    "        counter += 1\n",
    "    return rewardAll/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 6: 1, 8: 2, 9: 0, 10: 1, 13: 2, 14: 2}\n",
      "[ 0.01590991  0.01658739  0.05003502  0.01042981  0.01263779  0.\n",
      "  0.05589583  0.          0.02940273  0.08301691  0.23365018  0.          0.\n",
      "  0.27365458  0.62428006  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pi)\n",
    "# V(s)\n",
    "v_s = every_visit_monte_carlo_prediction_v(pi, env, 0, 10000)\n",
    "print (v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 6: 1, 8: 2, 9: 0, 10: 1, 13: 2, 14: 2}\n",
      "Q_SA\n",
      "[[ 0.          0.00870252  0.          0.        ]\n",
      " [ 0.          0.          0.0100307   0.        ]\n",
      " [ 0.          0.02381675  0.          0.        ]\n",
      " [ 0.01261329  0.          0.          0.        ]\n",
      " [ 0.          0.01366911  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.05763451  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.03017671  0.        ]\n",
      " [ 0.08761386  0.          0.          0.        ]\n",
      " [ 0.          0.18287804  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.26263611  0.        ]\n",
      " [ 0.          0.          0.5541214   0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "V\n",
      "[ 0.00870252  0.0100307   0.02381675  0.01261329  0.01366911  0.\n",
      "  0.05763451  0.          0.03017671  0.08761386  0.18287804  0.          0.\n",
      "  0.26263611  0.5541214   0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pi)\n",
    "q = every_visit_monte_carlo_prediction_q(pi,env,0, 10000)\n",
    "print('Q_SA')\n",
    "print(q)\n",
    "print('V')\n",
    "print(np.max(q,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 3, 0, 2, 1, 3, 3, 2, 1, 3, 2, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_new = np.random.randint(0,env.action_space.n,env.observation_space.n)\n",
    "policy_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward post episode 1000\n",
      "0.56\n",
      "Expected Reward post episode 2000\n",
      "0.6\n",
      "Expected Reward post episode 3000\n",
      "0.78\n",
      "Expected Reward post episode 4000\n",
      "0.75\n",
      "Expected Reward post episode 5000\n",
      "0.65\n",
      "Expected Reward post episode 6000\n",
      "0.63\n",
      "Expected Reward post episode 7000\n",
      "0.76\n",
      "Expected Reward post episode 8000\n",
      "0.72\n",
      "Expected Reward post episode 9000\n",
      "0.75\n",
      "Expected Reward post episode 10000\n",
      "0.64\n",
      "Expected Reward post episode 11000\n",
      "0.62\n",
      "Expected Reward post episode 12000\n",
      "0.72\n",
      "Expected Reward post episode 13000\n",
      "0.68\n",
      "Expected Reward post episode 14000\n",
      "0.81\n",
      "Expected Reward post episode 15000\n",
      "0.67\n",
      "Expected Reward post episode 16000\n",
      "0.74\n",
      "Expected Reward post episode 17000\n",
      "0.71\n",
      "Expected Reward post episode 18000\n",
      "0.68\n",
      "Expected Reward post episode 19000\n",
      "0.68\n",
      "Expected Reward post episode 20000\n",
      "0.75\n",
      "Expected Reward post episode 21000\n",
      "0.75\n",
      "Expected Reward post episode 22000\n",
      "0.72\n",
      "Expected Reward post episode 23000\n",
      "0.61\n",
      "Expected Reward post episode 24000\n",
      "0.69\n",
      "Expected Reward post episode 25000\n",
      "0.71\n",
      "Expected Reward post episode 26000\n",
      "0.73\n",
      "Expected Reward post episode 27000\n",
      "0.67\n",
      "Expected Reward post episode 28000\n",
      "0.68\n",
      "Expected Reward post episode 29000\n",
      "0.73\n",
      "Expected Reward post episode 30000\n",
      "0.74\n",
      "Expected Reward post episode 31000\n",
      "0.65\n",
      "Expected Reward post episode 32000\n",
      "0.71\n",
      "Expected Reward post episode 33000\n",
      "0.73\n",
      "Expected Reward post episode 34000\n",
      "0.66\n",
      "Expected Reward post episode 35000\n",
      "0.71\n",
      "Expected Reward post episode 36000\n",
      "0.69\n",
      "Expected Reward post episode 37000\n",
      "0.6\n",
      "Expected Reward post episode 38000\n",
      "0.73\n",
      "Expected Reward post episode 39000\n",
      "0.68\n",
      "Expected Reward post episode 40000\n",
      "0.67\n",
      "Expected Reward post episode 41000\n",
      "0.7\n",
      "Expected Reward post episode 42000\n",
      "0.68\n",
      "Expected Reward post episode 43000\n",
      "0.81\n",
      "Expected Reward post episode 44000\n",
      "0.7\n",
      "Expected Reward post episode 45000\n",
      "0.67\n",
      "Expected Reward post episode 46000\n",
      "0.71\n",
      "Expected Reward post episode 47000\n",
      "0.63\n",
      "Expected Reward post episode 48000\n",
      "0.71\n",
      "Expected Reward post episode 49000\n",
      "0.63\n",
      "Expected Reward post episode 50000\n",
      "0.75\n",
      "Expected Reward post episode 51000\n",
      "0.71\n",
      "Expected Reward post episode 52000\n",
      "0.66\n",
      "Expected Reward post episode 53000\n",
      "0.78\n",
      "Expected Reward post episode 54000\n",
      "0.67\n",
      "Expected Reward post episode 55000\n",
      "0.74\n",
      "Expected Reward post episode 56000\n",
      "0.69\n",
      "Expected Reward post episode 57000\n",
      "0.74\n",
      "Expected Reward post episode 58000\n",
      "0.74\n",
      "Expected Reward post episode 59000\n",
      "0.82\n",
      "Expected Reward post episode 60000\n",
      "0.74\n",
      "Expected Reward post episode 61000\n",
      "0.76\n",
      "Expected Reward post episode 62000\n",
      "0.83\n",
      "Expected Reward post episode 63000\n",
      "0.72\n",
      "Expected Reward post episode 64000\n",
      "0.74\n",
      "Expected Reward post episode 65000\n",
      "0.6\n",
      "Expected Reward post episode 66000\n",
      "0.75\n",
      "Expected Reward post episode 67000\n",
      "0.72\n",
      "Expected Reward post episode 68000\n",
      "0.73\n",
      "Expected Reward post episode 69000\n",
      "0.67\n",
      "Expected Reward post episode 70000\n",
      "0.73\n",
      "Expected Reward post episode 71000\n",
      "0.7\n",
      "Expected Reward post episode 72000\n",
      "0.71\n",
      "Expected Reward post episode 73000\n",
      "0.69\n",
      "Expected Reward post episode 74000\n",
      "0.67\n",
      "Expected Reward post episode 75000\n",
      "0.73\n",
      "Expected Reward post episode 76000\n",
      "0.75\n",
      "Expected Reward post episode 77000\n",
      "0.67\n",
      "Expected Reward post episode 78000\n",
      "0.75\n",
      "Expected Reward post episode 79000\n",
      "0.62\n",
      "Expected Reward post episode 80000\n",
      "0.71\n",
      "Expected Reward post episode 81000\n",
      "0.67\n",
      "Expected Reward post episode 82000\n",
      "0.67\n",
      "Expected Reward post episode 83000\n",
      "0.73\n",
      "Expected Reward post episode 84000\n",
      "0.65\n",
      "Expected Reward post episode 85000\n",
      "0.69\n",
      "Expected Reward post episode 86000\n",
      "0.64\n",
      "Expected Reward post episode 87000\n",
      "0.71\n",
      "Expected Reward post episode 88000\n",
      "0.7\n",
      "Expected Reward post episode 89000\n",
      "0.75\n",
      "Expected Reward post episode 90000\n",
      "0.73\n",
      "Expected Reward post episode 91000\n",
      "0.67\n",
      "Expected Reward post episode 92000\n",
      "0.76\n",
      "Expected Reward post episode 93000\n",
      "0.72\n",
      "Expected Reward post episode 94000\n",
      "0.64\n",
      "Expected Reward post episode 95000\n",
      "0.74\n",
      "Expected Reward post episode 96000\n",
      "0.64\n",
      "Expected Reward post episode 97000\n",
      "0.73\n",
      "Expected Reward post episode 98000\n",
      "0.73\n",
      "Expected Reward post episode 99000\n",
      "0.72\n",
      "Expected Reward post episode 100000\n",
      "0.65\n"
     ]
    }
   ],
   "source": [
    "n,q,p = montecarlo_control(env,nb_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 2, 3, 3, 0, 2, 1, 1, 3, 0, 3, 1, 0, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,env.action_space.n,env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Reward post episode 1000\n",
      "0.51\n",
      "Expected Reward post episode 2000\n",
      "0.42\n",
      "Expected Reward post episode 3000\n",
      "0.44\n",
      "Expected Reward post episode 4000\n",
      "0.55\n",
      "Expected Reward post episode 5000\n",
      "0.51\n",
      "Expected Reward post episode 6000\n",
      "0.59\n",
      "Expected Reward post episode 7000\n",
      "0.5\n",
      "Expected Reward post episode 8000\n",
      "0.45\n",
      "Expected Reward post episode 9000\n",
      "0.52\n",
      "Expected Reward post episode 10000\n",
      "0.57\n",
      "Expected Reward post episode 11000\n",
      "0.49\n",
      "Expected Reward post episode 12000\n",
      "0.47\n",
      "Expected Reward post episode 13000\n",
      "0.45\n",
      "Expected Reward post episode 14000\n",
      "0.48\n",
      "Expected Reward post episode 15000\n",
      "0.46\n",
      "Expected Reward post episode 16000\n",
      "0.5\n",
      "Expected Reward post episode 17000\n",
      "0.51\n",
      "Expected Reward post episode 18000\n",
      "0.5\n",
      "Expected Reward post episode 19000\n",
      "0.53\n",
      "Expected Reward post episode 20000\n",
      "0.54\n",
      "Expected Reward post episode 21000\n",
      "0.54\n",
      "Expected Reward post episode 22000\n",
      "0.54\n",
      "Expected Reward post episode 23000\n",
      "0.42\n",
      "Expected Reward post episode 24000\n",
      "0.54\n",
      "Expected Reward post episode 25000\n",
      "0.62\n",
      "Expected Reward post episode 26000\n",
      "0.51\n",
      "Expected Reward post episode 27000\n",
      "0.54\n",
      "Expected Reward post episode 28000\n",
      "0.66\n",
      "Expected Reward post episode 29000\n",
      "0.76\n",
      "Expected Reward post episode 30000\n",
      "0.65\n",
      "Expected Reward post episode 31000\n",
      "0.7\n",
      "Expected Reward post episode 32000\n",
      "0.73\n",
      "Expected Reward post episode 33000\n",
      "0.77\n",
      "Expected Reward post episode 34000\n",
      "0.8\n",
      "Expected Reward post episode 35000\n",
      "0.74\n",
      "Expected Reward post episode 36000\n",
      "0.76\n",
      "Expected Reward post episode 37000\n",
      "0.7\n",
      "Expected Reward post episode 38000\n",
      "0.74\n",
      "Expected Reward post episode 39000\n",
      "0.73\n",
      "Expected Reward post episode 40000\n",
      "0.72\n",
      "Expected Reward post episode 41000\n",
      "0.72\n",
      "Expected Reward post episode 42000\n",
      "0.76\n",
      "Expected Reward post episode 43000\n",
      "0.7\n",
      "Expected Reward post episode 44000\n",
      "0.64\n",
      "Expected Reward post episode 45000\n",
      "0.69\n",
      "Expected Reward post episode 46000\n",
      "0.75\n",
      "Expected Reward post episode 47000\n",
      "0.74\n",
      "Expected Reward post episode 48000\n",
      "0.73\n",
      "Expected Reward post episode 49000\n",
      "0.65\n",
      "Expected Reward post episode 50000\n",
      "0.71\n",
      "Expected Reward post episode 51000\n",
      "0.71\n",
      "Expected Reward post episode 52000\n",
      "0.71\n",
      "Expected Reward post episode 53000\n",
      "0.74\n",
      "Expected Reward post episode 54000\n",
      "0.68\n",
      "Expected Reward post episode 55000\n",
      "0.75\n",
      "Expected Reward post episode 56000\n",
      "0.83\n",
      "Expected Reward post episode 57000\n",
      "0.77\n",
      "Expected Reward post episode 58000\n",
      "0.77\n",
      "Expected Reward post episode 59000\n",
      "0.7\n",
      "Expected Reward post episode 60000\n",
      "0.7\n",
      "Expected Reward post episode 61000\n",
      "0.71\n",
      "Expected Reward post episode 62000\n",
      "0.73\n",
      "Expected Reward post episode 63000\n",
      "0.81\n",
      "Expected Reward post episode 64000\n",
      "0.78\n",
      "Expected Reward post episode 65000\n",
      "0.75\n",
      "Expected Reward post episode 66000\n",
      "0.73\n",
      "Expected Reward post episode 67000\n",
      "0.7\n",
      "Expected Reward post episode 68000\n",
      "0.68\n",
      "Expected Reward post episode 69000\n",
      "0.65\n",
      "Expected Reward post episode 70000\n",
      "0.71\n",
      "Expected Reward post episode 71000\n",
      "0.79\n",
      "Expected Reward post episode 72000\n",
      "0.74\n",
      "Expected Reward post episode 73000\n",
      "0.77\n",
      "Expected Reward post episode 74000\n",
      "0.71\n",
      "Expected Reward post episode 75000\n",
      "0.68\n",
      "Expected Reward post episode 76000\n",
      "0.76\n",
      "Expected Reward post episode 77000\n",
      "0.77\n",
      "Expected Reward post episode 78000\n",
      "0.84\n",
      "Expected Reward post episode 79000\n",
      "0.74\n",
      "Expected Reward post episode 80000\n",
      "0.82\n",
      "Expected Reward post episode 81000\n",
      "0.78\n",
      "Expected Reward post episode 82000\n",
      "0.76\n",
      "Expected Reward post episode 83000\n",
      "0.69\n",
      "Expected Reward post episode 84000\n",
      "0.67\n",
      "Expected Reward post episode 85000\n",
      "0.69\n",
      "Expected Reward post episode 86000\n",
      "0.71\n",
      "Expected Reward post episode 87000\n",
      "0.73\n",
      "Expected Reward post episode 88000\n",
      "0.65\n",
      "Expected Reward post episode 89000\n",
      "0.73\n",
      "Expected Reward post episode 90000\n",
      "0.69\n",
      "Expected Reward post episode 91000\n",
      "0.74\n",
      "Expected Reward post episode 92000\n",
      "0.75\n",
      "Expected Reward post episode 93000\n",
      "0.79\n",
      "Expected Reward post episode 94000\n",
      "0.72\n",
      "Expected Reward post episode 95000\n",
      "0.73\n",
      "Expected Reward post episode 96000\n",
      "0.69\n",
      "Expected Reward post episode 97000\n",
      "0.78\n",
      "Expected Reward post episode 98000\n",
      "0.81\n",
      "Expected Reward post episode 99000\n",
      "0.8\n",
      "Expected Reward post episode 100000\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "n,q,p = montecarlo_control(env, nb_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
