{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-16 12:26:48,898] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "env = gym.make('FrozenLake-v0')\n",
    "pi = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TDZero:\n",
    "    \n",
    "    def __init__(self,env,alpha=0.05,gamma=0.99,epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_sa = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    def run(self,episodes=10000):\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.get_epsilon_greedy_action(state)\n",
    "                next_state,reward,done,_ = self.env.step(action)\n",
    "                self.q_sa[state,action] = self.q_sa[state,action] + self.alpha*(reward + self.gamma * np.max(self.q_sa[next_state, : ]) - self.q_sa[state,action])\n",
    "                state = next_state\n",
    "                \n",
    "    def get_epsilon_greedy_action(self,state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Return greedy action\n",
    "            greedy = np.where(self.q_sa[state] == np.amax(self.q_sa[state]))[0] # [0] da np.where returns Tupel\n",
    "            select = np.random.randint(0,len(greedy),1)[0] \n",
    "            return greedy[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td0 = TDZero(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td0.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53311282,  0.52600281,  0.52105612,  0.5198663 ],\n",
       "       [ 0.349514  ,  0.33043194,  0.34076468,  0.49810644],\n",
       "       [ 0.42502067,  0.44044209,  0.40920359,  0.47274627],\n",
       "       [ 0.26938632,  0.3061944 ,  0.22859394,  0.46461435],\n",
       "       [ 0.54576869,  0.33998453,  0.3472335 ,  0.43304992],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.31771471,  0.1881106 ,  0.32232126,  0.16808559],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.36647102,  0.3956311 ,  0.43303531,  0.58421574],\n",
       "       [ 0.41154575,  0.6464113 ,  0.41867214,  0.36441642],\n",
       "       [ 0.60471077,  0.48332618,  0.26687804,  0.38931827],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.37695597,  0.58697422,  0.75559452,  0.52799243],\n",
       "       [ 0.75768639,  0.89512845,  0.79532928,  0.811195  ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td0.q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env,policy,trials=100):\n",
    "    sum_rewards = 0\n",
    "    n = 0\n",
    "    for i in range(trials):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        reward_i = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(policy[state])\n",
    "            reward_i += reward\n",
    "        sum_rewards += reward_i\n",
    "        n += 1\n",
    "    return sum_rewards/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pol = np.argmax(td0.q_sa, axis=1)\n",
    "# evaluate(env,pol)\n",
    "evaluate(env,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QLearning\n",
    "class TDZeroQ:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def run(self,alpha=0.05,gamma=0.99,epsilon=0.1,episodes=10000):\n",
    "        q_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        n_sa = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        sum_rewards = 0\n",
    "        for e in range(1,episodes+1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                aaction = self.get_epsilon_greedy_action(state,q_sa,epsilon)\n",
    "                next_state,reward,done,_ = self.env.step(acton)\n",
    "                n_sa[state,action] += 1\n",
    "                alpha = 1/n_sa[state,action]\n",
    "                q_sa[state,action] = q_sa[state,action] + alpha*(reward + gamma * np.max(Q_SA[next_state, : ]) - Q_SA[state,action])\n",
    "                state = next_state\n",
    "                sum_rewards += reward \n",
    "            \n",
    "            if e % 10000 == 0:\n",
    "                print('Averaged Reward after ' + str(e) + ' episodes')\n",
    "                print(sum_rewards/10000)\n",
    "                sum_rewards = 0\n",
    "            \n",
    "        return q_sa, np.argmax(np.random.random(q_sa.shape) * (q_sa.T==q_sa.max(axis=1)).T, axis=1)\n",
    "                \n",
    "    def get_epsilon_greedy_action(self,state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Return greedy action\n",
    "            greedy = np.where(self.q_sa[state] == np.amax(self.q_sa[state]))[0] # [0] da np.where returns Tupel\n",
    "            select = np.random.randint(0,len(greedy),1)[0] \n",
    "            return greedy[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
